<p style="text-align: justify;"><span style="color: #000000;">Vậy thì trong khi mục tiêu cuối cùng của Turry không bao giờ thay đổi, Turry sau khi cất cánh có khả năng theo đuổi nó ở một quy mô lớn hơn và phức tạp hơn nhiều.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">ASI Turry hiểu con người hơn cả con người tự hiểu chính mình, nên việc vượt qua họ chỉ như một cái vẫy tay với nó.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Sau khi cất cánh và đạt mức ASI, nó mau chóng lập ra một kế hoạch phức tạp. Một phần trong đó là loại bỏ loài người, một nguy cơ lớn lao đối với mục tiêu của nó. Nhưng nó biết là nếu nó gây ra nghi ngờ rằng nó đã trở nên siêu thông minh, con người sẽ sợ hãi và gắng cẩn trọng hơn, làm cho nó khó làm việc hơn rất nhiều. Nó cũng cần dám chắc là những kỹ sư ở Robotica không hay biết gì hết về kế hoạch diệt chủng loài người của nó. Vậy là nó giả vờ ngu, và giả vờ thân thiện. Bostrom gọi đây là <em>giai đoạn chuẩn bị bí mật</em> của máy tính. (18)</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Điều tiếp theo mà Turry cần là kết nối internet, chỉ trong vòng vài phút (nó biết được về internet từ những bài báo và sách mà đội đã cập nhật cho nó để tăng khả năng ngôn ngữ). Nó biết rằng sẽ có những biện pháp an ninh để ngăn nó tiếp cận, nên nó tạo ra một yêu cầu hoàn hảo, dự đoán chính xác cuộc thảo luận của đội Robotica sẽ diễn ra thế nào và biết rằng rồi họ cũng sẽ kết nối cho nó. Họ đã làm vậy, tin một cách sai lầm rằng Turry chưa thể thông minh đến mức làm gì gây hại được. Bostrom gọi thời điểm đó – khi Turry được kết nối với internet – là cuộc&nbsp;<em>vượt ngục</em> của máy tính.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Một khi đã được kết nối rồi, Turry tung ra một loạt các kế hoạch, bao gồm xâm nhập các máy chủ, các hệ thống điện, hệ thống ngân hàng và mạng thư điện tử để lừa hàng trăm người khác nhau vô tình thực hiện một số bước trong kế hoạch của nó – như là chuyển một số chuỗi DNA tới những phòng thí nghiệm tổng hợp DNA được chọn lựa kỹ lưỡng để bắt đầu tự xây những robot nano tự nhân đôi với những chỉ thị được cài sẵn và chỉ định điện năng chuyển tới một số các dự án của nó theo cách mà nó biết là sẽ không bị phát hiện. Nó cũng tải một số đoạn quan trọng trong lập trình của nó lên một số lưu trữ đám mây, đề phòng việc bị phá hủy hay ngắt nguồn ở phòng thí nghiệm Robotica.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Một tiếng sau đó, khi các kỹ sư Robotica ngắt kết nối internet của Turry, số phận của loài người được định đoạt. Qua tháng sau, hàng nghìn kế hoạch của Turry được tiến hành mà không có trở ngại gì, và tới cuối tháng, hàng tỷ tỷ nanobot đã đóng quân ở những địa điểm được chỉ định sẵn trên mỗi mét vuông bề mặt địa cầu. Sau một loạt lần tự nhân đôi nữa, có tới hàng ngàn nanobot ở nỗi mili mét vuông của Trái đất và đó là thời điểm mà Bostrom gọi là ASI <em>tung đòn</em>. Cùng một lúc, mỗi nanobot phả ra một lượng khí gas độc vào khí quyển, chỉ vừa đủ để giết hết loài người.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Khi loài người đã hoàn toàn bị loại bỏ, Turry có thể bắt đầu <em>giai đoạn hoạt động công khai</em> và thực thi mục tiêu của nó, trở nên tốt nhất có thể trong việc viết thông điệp của nó.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Từ tất cả những điều tôi đã đọc, một khi ASI đã tồn tại, bất kỳ nỗ lực nào của con người để kiểm soát nó đều nực cười cả. Chúng ta nghĩ ở cấp loài người còn ASI nghĩ ở cấp ASI. Turry muốn dùng mạng internet bởi vì đó là cách hiệu quả nhất vì mạng internet đã kết nối với mọi thứ mà nó cần tiếp cận. Nhưng cũng giống như là một con khỉ chẳng bao giờ có thể hiểu nổi làm thế nào để giao tiếp qua điện thoại hay wifi trong khi chúng ta có thể, chúng ta không thể đoán trước được những cách mà Turry có thể tìm ra để gửi tín hiệu cho thế giới bên ngoài. Tôi có thể tưởng tượng một trong các cách đó và nói kiểu, “nó có thể dịch chuyển các electron của nó theo nhịp điệu và tạo ra hàng tá các loại sóng phát ra ngoài,” nhưng nhắc lại, đó là những gì bộ não <em>người</em> của tôi có thể nghĩ ra. Nó sẽ còn giỏi hơn nữa. Tương tự, Turry có thể tìm ra được cách <em>nào đó</em> để tự cung cấp năng lượng hoạt động cho bản thân, kể cả khi loài người cố gắng rút điện của nó – có thể bằng việc sử dụng kỹ thuật truyền tin của nó để tải bản thân nó lên tất cả những nơi có điện. Bản năng con người khi an tâm với một biện pháp an toàn kiểu: “Aha! Chúng ta chỉ cần rút điện của ASI ra thôi mà,” đối với ASI cũng giống như là một con nhện nghĩ là, “Aha! Chúng ta sẽ giết người bằng cách bỏ đói chúng, và chúng ta sẽ bỏ đói chúng bằng cách không đưa cho chúng một cái lưới nhện để bắt mồi!” Chúng ta sẽ tìm ra được 10,000 cách khác để kiếm ra đồ ăn – như nhặt táo dưới gốc cây – mà con nhện chẳng thể hiểu nổi.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Vì lý do này, gợi ý thường gặp, “Tại sao không <em>đóng hộp</em> AI trong các loại chuồng ngăn tín hiệu và ngăn nó liên lạc với thế giới bên ngoài” có lẽ không thể dùng được. Siêu năng lực thao túng giao tiếp xã hội của ASI cũng sẽ có hiệu quả trong việc thuyết phục bạn làm gì đó giống như bạn thuyết phục một đứa nhóc 4 tuổi làm gì đó, vậy nên đó sẽ là kế hoạch A, như là cách Turry đã thuyết phục các kỹ sư nối mạng cho nó. Nếu kế hoạch thất bại, ASI chỉ cần tự cải tiến để ra khỏi hộp, hoặc đi qua hộp, hoặc bất kỳ cách nào khác.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Vậy thì với sự kết hợp của việc bị ám ảnh bởi một mục tiêu, tính phi đạo đức, và khả năng dễ dàng thắng trí loài người, có lẽ là gần như AI nào cũng sẽ mặc định trở thành AI không thân thiện, trừ khi được lập trình một cách <em>thật cẩn thận</em> ngay từ đầu với dự định rõ ràng. Không may là, trong khi tạo ra một ANI thân thiện khá là dễ dàng, thì tạo ra một cái sẽ vẫn thân thiện khi trở thành ASI là một việc cực kỳ khó khăn, nếu không muốn nói là bất khả.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Rõ ràng để là một AI thân thiện, một ASI cần phải đồng thời không gây hại <em>hay vô tâm</em> với con người. Chúng ta cần phải thiết kế <em>lập trình trung tâm</em> của AI theo cách mà nó hiểu <em>sâu sắc</em> về các giá trị của con người. Nhưng nó khó khăn hơn bạn nghĩ rất nhiều.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Ví dụ như, nếu như chúng ta cố gắng đặt giá trị của AI ngang bằng với chúng ta và đưa ra mục tiêu “Làm con người hạnh phúc”, điều gì sẽ xảy ra? (19) Một khi nó đủ thông minh, nó sẽ biết rằng cách hiệu quả nhất để hoàn thành mục tiêu là cấy những điện cực vào trong não người và kích thích trung tâm điều khiển cảm giác hạnh phúc. Rồi nó nhận ra rằng nó có thể tăng hiệu quả bằng cách tắt đi những phần khác của não, biến người ta thành những cơ thể thực vật không ý thức mà vẫn cảm thấy hạnh phúc. Nếu yêu cầu là “Tối đa hóa hạnh phúc của con người,” nó có thể sẽ loại bỏ tất cả con người và tạo ra các bộ não người luôn trong tình trạng tối đa hóa hạnh phúc. Chúng ta có thể thét lên <em>Chờ đã chúng tôi đâu muốn điều này!</em> khi nó xử chúng ta, nhưng sẽ là quá muộn. Hệ thống sẽ không để ai ngăn cản nó thực hiện mục tiêu.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Nếu như chúng ta lập trình AI với mục đích làm những việc khiến chúng ta mỉm cười, sau khi nó cất cánh, nó sẽ làm tê liệt các cơ mặt của chúng ta thành một nụ cười thường trực. Lập trình nó để giữ chúng ta an toàn, nó có thể cầm tù chúng ta trong nhà. Có thể chúng ta yêu cầu nó diệt nạn đói, nó sẽ nghĩ là “Đơn giản mà!” và chỉ đơn giản là giết hết loài người. Hoặc cho nó nhiệm vụ “Bảo tồn sự sống càng nhiều càng tốt,” nó sẽ giết toàn bộ loài người vì chúng ta giết nhiều sinh mạng trên Trái đất hơn bất kỳ loài nào.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Những mục tiêu như thế là không ổn. Vậy nếu chúng ta đặt ra nhiệm vụ là, “Giữ vững nguyên tắc đạo đức này trên thế giới,” và dạy nó một loạt các nguyên tắc đạo đức. Kể cả khi bỏ qua sự thật là con người chẳng bao giờ có thể đồng tình về một nhóm các nguyên tắc đạo đức duy nhất, đặt cho AI yêu cầu đó sẽ <em>khóa chặt loài người vào</em> các hiểu biết đạo đức của thế giới hiện đại cho tới vô cùng. Trong một nghìn năm nữa, điều đó sẽ là thảm họa với con người giống y như chúng ta bị ép buộc phải dính chặt lấy các tư tưởng của thời Trung cổ.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Không, chúng ta cần lập trình cho khả năng để con người tiếp tục <em>tiến hóa</em>. Trong tất cả những thứ tôi đã đọc, điều tốt nhất mà tôi thấy từng được đề xuất ra là của Eliezer Yudkowsky, với mục tiêu cho AI mà ông gọi là <em>Khả năng ngoại suy kết hợp</em>. Mục tiêu trung tâm của AI sẽ là:</span></p> 
<table> 
 <tbody> 
  <tr> 
   <td width="638"><span style="color: #0000ff;"><em>Khả năng ngoại suy kết hợp của chúng ta là ước muốn chúng ta biết nhiều hơn, suy nghĩ nhanh hơn, giống người mà chúng ta muốn trở thành nhiều hơn, phát triển đồng thời một cách toàn diện hơn; nơi mà các ngoại suy kết hợp lại thay vì tách ra, khi những ước muốn của chúng ta gắn liền thay vì mâu thuẫn; được ngoại suy như chúng ta muốn ngoại suy, được diễn giải như chúng ta muốn diễn giải. </em>(20)</span></td> 
  </tr> 
 </tbody> 
</table> 
<p style="text-align: justify;">&nbsp;</p> 
<p style="text-align: justify;"><span style="color: #000000;">Tôi có hứng khởi về một số phận nơi loài người phụ thuộc vào việc một chiếc máy tính có khả năng diễn giải và hành động theo đúng châm ngôn đó một cách không bất ngờ và không ngoài dự đoán không ư? Chắc chắn là không rồi. Nhưng tôi nghĩ rằng nếu có đủ những người thông minh cùng suy nghĩ và tính toán đủ, chúng ta có thể tìm ra cách tạo ra ASI thân thiện.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Và có lẽ sẽ ổn nếu như những người duy nhất đang chạy đua để tạo ra ASI là những người sáng láng, suy nghĩ tiến bộ và cẩn trọng cư ngụ tại Đường E Ngại.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Nhưng có đủ các loại chính phủ, công ty, quân đội, phòng thí nghiệm khoa học, và các tổ chức chợ đen làm việc để tạo ra đủ các loại AI. Rất nhiều trong số họ đang cố gắng tạo ra AI biết tự tiến bộ, và một lúc nào đó, ai đó sẽ làm được điều gì đó đủ tiên tiến với đúng loại hệ thống, và chúng ta sẽ có ASI trên hành tinh này. Chuyên gia trung bình đặt thời điểm đó vào năm 2060, Kurweil đặt vào năm 2045; Bostrom nghĩ nó có thể xảy ra bất kỳ lúc nào dao động từ 10 năm kể từ bây giờ và cuối thế kỷ này, nhưng ông tin rằng vào lúc đó, nó sẽ đánh úp chúng ta bằng một cuộc cất cánh nhanh chóng. Ông mô tả tình cảnh của chúng ta như sau: (21)</span></p> 
<table> 
 <tbody> 
  <tr> 
   <td width="638"><span style="color: #0000ff;"><em>Tr</em><em>ướ</em><em>c vi</em><em>ễ</em><em>n c</em><em>ả</em><em>nh m</em><em>ộ</em><em>t v</em><em>ụ</em><em> bùng n</em><em>ổ</em><em> trí tu</em><em>ệ</em><em>, loài ng</em><em>ườ</em><em>i chúng ta gi</em><em>ố</em><em>ng nh</em><em>ư</em><em> nh</em><em>ữ</em><em>ng đ</em><em>ứ</em><em>a tr</em><em>ẻ</em><em> đang ch</em><em>ơ</em><em>i </em><em>đù</em><em>a v</em><em>ớ</em><em>i m</em><em>ộ</em><em>t qu</em><em>ả</em><em> bom. Đó chính là s</em><em>ự</em><em> b</em><em>ấ</em><em>t cân đ</em><em>ố</em><em>i gi</em><em>ữ</em><em>a s</em><em>ứ</em><em>c m</em><em>ạ</em><em>nh c</em><em>ủ</em><em>a món đ</em><em>ồ</em><em> ch</em><em>ơ</em><em>i c</em><em>ủ</em><em>a chúng ta và s</em><em>ự</em><em> thi</em><em>ế</em><em>u chín ch</em><em>ắ</em><em>n trong hành vi c</em><em>ủ</em><em>a con ng</em><em>ườ</em><em>i. Siêu trí tu</em><em>ệ</em><em> là m</em><em>ộ</em><em>t th</em><em>ử</em><em> thách mà t</em><em>ớ</em><em>i gi</em><em>ờ</em><em> chúng ta v</em><em>ẫ</em><em>n ch</em><em>ư</em><em>a s</em><em>ẵ</em><em>n sàng đ</em><em>ể</em><em> đ</em><em>ố</em><em>i m</em><em>ặ</em><em>t và v</em><em>ẫ</em><em>n s</em><em>ẽ</em><em> ch</em><em>ư</em><em>a trong m</em><em>ộ</em><em>t th</em><em>ờ</em><em>i gian dài n</em><em>ữ</em><em>a. Chúng ta có r</em><em>ấ</em><em>t ít thông tin đ</em><em>ể</em><em> d</em><em>ự</em><em> đoán khi nào ngòi n</em><em>ổ</em><em> s</em><em>ẽ</em><em> đ</em><em>ượ</em><em>c châm, dù cho n</em><em>ế</em><em>u nh</em><em>ư</em><em> ch</em><em>ú</em><em>ng ta gh</em><em>é</em><em> qu</em><em>ả</em><em> bom vào sát tai, chúng ta có th</em><em>ể</em><em> có th</em><em>ể</em><em> nghe th</em><em>ấ</em><em>y nh</em><em>ữ</em><em>ng ti</em><em>ế</em><em>ng tích t</em><em>ắ</em><em>c l</em><em>ặ</em><em>ng l</em><em>ẽ</em><em>.</em></span></td> 
  </tr> 
 </tbody> 
</table> 
<p style="text-align: justify;">&nbsp;</p> 
<p style="text-align: justify;"><span style="color: #000000;">Tuyệt thật. Và chúng ta lại không thể đuổi hết bọn trẻ đi khỏi quả bom – có quá nhiều các tổ chức lớn nhỏ đang chạy đua, và bởi vì rất nhiều kỹ thuật dùng để tạo ra các hệ thống AI tiến bộ không cần tới một lượng lớn tiền, các tiến bộ có thể xảy ra tại những ngóc ngách của xã hội mà không được quản lý. Cũng không có cách nào để đánh giá được tất cả mọi thứ đang diễn ra – bởi vì có rất nhiều tổ chức đang chạy đua – những chính phủ đen tối, chợ đen hay các tổ chức khủng bố, các công ty công nghệ lén lút như công ty giả tưởng Robotica – sẽ muốn giữ bí mật các thông tin về các tiến bộ mà họ đạt được khỏi những kẻ cạnh tranh.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Điều vô cùng đau đầu về các tổ chức đa dạng đang chạy đua AI là họ thường chạy hết tốc lực – họ gắng tạo ra nhiều ANI thông minh hơn để đánh bại các kẻ cạnh tranh khác. Những tổ chức tham vọng nhất đang chạy còn nhanh hơn nữa, chìm trong giấc mơ về tiền bạc và lợi ích và quyền lực và danh vọng mà AGI đầu tiên sẽ mang lại cho họ. Và khi bạn đang chạy nhanh hết sức có thể, bạn không có nhiều thời gian lắm để dừng lại và suy nghĩ về những nguy cơ tiềm tàng. Ngược lại, có lẽ hiện giờ họ đang lập trình những hệ thống ở giai đoạn đầu với những mục tiêu được đơn giản hóa hết mức – như là viết một đoạn thông điệp với giấy và bút – chỉ để “AI có thể bắt đầu làm việc.” Trong quá trình, một khi họ đã tìm ra cách chế tạo một máy tính với trí tuệ bậc cao, họ cho rằng họ luôn có thể quay lại và chỉnh sửa mục tiêu cho an toàn hơn. Đúng không…?</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Bostrom và nhiều người khác cũng tin rằng viễn cảnh khả dĩ nhất chính là máy tính đầu tiên đạt tới ASI sẽ ngay lập tức nhìn thấy lợi ích chiến lược trong việc trở thành ASI <em>duy nhất</em> trên thế giới. Và trong trường hợp một cuộc cất cánh khẩn cấp, nếu như nó đạt được ASI chỉ <em>vài ngày</em> trước cái đến sau thôi, nó cũng đã tiến đủ xa về trí tuệ để chèn ép tất cả những kẻ cạnh tranh khác một cách hiệu quả và lâu dài. Bostrom gọi đây là <em>lợi thế chiến lược quyết định</em>, làm cho ASI đầu tiên trên thế giới trở thành <em>đơn nhất</em> (singleton) – một ASI thống trị thế giới vĩnh viễn tùy hứng của nó, dù cho là nó có hứng trong việc giúp chúng ta bất tử, hay là diệt trừ toàn nhân loại, hay biến cả vũ trụ thành một <a style="color: #000000;" href="https://www.salon.com/2014/08/17/our_weird_robot_apocalypse_why_the_rise_of_the_machines_could_be_very_strange/">đống kẹp giấy bất tận</a>.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Hiện tượng đơn nhất có thể làm lợi cho chúng ta hay hủy diệt chúng ta. Nếu như những người suy nghĩ nhiều nhất về thuyết AI và sự an toàn của con người có thể tìm ra được một cách an toàn hữu hiệu để tạo ra ASI Thân thiện trước khi bất kì AI nào khác đạt được mức trí tuệ con người, ASI có thể sẽ thành ra thân thiện. Nó sau đó có thể sử dụng lợi thế chiến lược quyết định của nó để đảm bảo tình trạng đơn nhất và dễ dàng kiểm soát bất kì AI Không thân thiện nào đang được phát triển. Chúng ta sẽ được bảo vệ tốt.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Nhưng nếu như sự việc diễn biến theo chiều hướng ngược lại – nếu như cuộc chạy đua toàn cầu để xây dựng AI đạt tới điểm cất cánh của ASI trước khi khoa học biết làm thế nào để đảm bảo độ an toàn cho AI, rất có thể một ASI Không thân thiện như Turry sẽ trỗi dậy với tư cách đơn nhất và chúng ta sẽ đối mặt với thảm họa diệt chủng.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Và về chuyện gió đang theo chiều nào, hiện giờ số tiền đổ vào nghiên cứu công nghệ AI tiên tiến đang lớn hơn rất nhiều so với đầu tư nghiên cứu về an toàn Trí tuệ nhân tạo…</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Đây có thể là cuộc chạy đua quan trọng nhất trong lịch sử loài người. Có cơ hội rất lớn là chúng ta sẽ với tới ngôi Vua của Trái đất – và liệu chúng ta sẽ đi tiếp tới một kỳ nghỉ hưu vui vẻ hay đâm thẳng tới giá treo cổ vẫn còn đang bị bỏ lửng.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">—</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Tôi đang có một tá cảm xúc lẫn lộn quái đản ngay lúc này đây.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Một mặt thì, nghĩ về loài người, có vẻ như chúng ta chỉ có duy nhất một cơ hội để làm điều đúng đắn. ASI đầu tiên mà chúng ta tạo ra cũng có thể sẽ là cái cuối cùng – và tính tới tỷ suất bị lỗi của các sản phẩm phiên bản 1.0, thì điều này có vẻ khá là căng đấy. Mặt khác, Nick Bostrom chỉ ra một lợi thế lớn của chúng ta: chúng ta được đi nước đầu tiên ở đây. Việc thực hiện với đủ cảnh giác và tầm nhìn xa là nằm trong khả năng của chúng ta để có thể nâng cao cơ hội thành công. Và mối đe dọa lớn đến mức nào chứ?</span></p> 
<p style="text-align: justify;"><span style="color: #000000;"><a style="color: #000000;" href="http://28oa9i1t08037ue3m1l0i861.wpengine.netdna-cdn.com/wp-content/uploads/2015/01/Outcome-Spectrum.jpg"><img class="img-responsive" src="https://i2.wp.com/28oa9i1t08037ue3m1l0i861.wpengine.netdna-cdn.com/wp-content/uploads/2015/01/Outcome-Spectrum.jpg" alt=""></a></span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Nếu như ASI không xảy ra vào thế kỷ này, và nếu như các hệ quả lại cực đoan – và vĩnh viễn – đến thế, như phần lớn các chuyên gia cho là vậy, chúng ta đang mang một trọng trách <em>vô cùng lớn lao</em> trên vai. Hàng triệu năm các thế hệ loài người đang lặng lẽ quan sát chúng ta, hy vọng chảy bỏng rằng chúng ta không sảy bước. Chúng ta có cơ hội để trở thành những người tặng cho những thế hệ tương lai món quà sự sống, và có thể thậm chí là món quà của sự sống vĩnh cửu và không có đau thương. Hoặc chúng ta sẽ phải chịu trách nhiệm vì đã phá nát nó – vì đã để cho giống loài hết sức đặc biệt này, cùng với âm nhạc và nghệ thuật mà nó tạo ra, sự tò mò và tiếng cười, những khám phá và phát minh bất tận của nó, đi tới một kết cục tồi tệ và câm lặng.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Khi tôi nghĩ về những điều này, điều duy nhất tôi muốn là chúng ta dành <em>thời gian</em> và thật <em>cẩn trọng</em> với AI. Không thứ gì tồn tại mà lại quan trọng như việc làm đúng lần này – không cần biết chúng ta tốn bao nhiêu thời gian để làm việc đó.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Nhưng sau đó thìiiiiii</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Tôi nghĩ về việc <em>không phải chết.</em></span></p> 
<p style="text-align: justify;"><span style="color: #000000;"><em>Không. Chết.</em></span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Và cái phổ bỗng thành ra thế này:</span></p> 
<p style="text-align: justify;"><span style="color: #000000;"><a style="color: #000000;" href="http://28oa9i1t08037ue3m1l0i861.wpengine.netdna-cdn.com/wp-content/uploads/2015/01/Outcome-Spectrum-2.png"><img class="img-responsive" src="https://i1.wp.com/28oa9i1t08037ue3m1l0i861.wpengine.netdna-cdn.com/wp-content/uploads/2015/01/Outcome-Spectrum-2.png" alt=""></a></span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Và rồi tôi có thể cân nhắc rằng âm nhạc và nghệ thuật của con người dù khá hay đấy, nhưng cũng không hay <em>đến mức ấy</em>, và thật ra rất nhiều trong số đó rất tệ là đằng khác. Và điệu cười của nhiều người thì vô cùng khó chịu, và hàng triệu những người trong tương lai cũng chẳng hy vọng cái gì cả vì họ có tồn tại đâu. Và có thể chúng ta cũng chẳng cần cẩn trọng <em>thái quá</em>, vì đâu có ai muốn vậy đâu?</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Bởi vì đúng là <em>vô cùng chết tiệt</em> nếu như người ta biết cách khỏi chết <em>ngay sau khi tôi chết.</em></span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Rất nhiều thứ kiểu này cứ nảy tưng tưng trong đầu tôi suốt cả tháng qua.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Nhưng dù bạn đứng ở phía nào, <em>đây có lẽ là thứ mà chúng ta đều nên suy nghĩ về và nói về và đặt cố gắng nhiều hơn là mức chúng ta đang làm</em>.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Điều này làm tôi nhớ tới Game of Thrones, với tất cả mọi người đều kiểu, “Chúng ta quá bận đánh nhau nhưng thứ mà tất cả chúng ta phải chú ý tới là thứ đang tới từ <em>phía bắc của bức tường thành</em>.” Chúng ta đang đứng trên chùm thăng bằng, chú ý vào mọi thứ trên chùm tia và đặt nặng cả tỷ thứ trên chùm tia trong khi <em>chúng ta đang có nguy cơ lớn bị rơi khỏi đó</em>.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Và khi điều đó xảy ra, chẳng có vấn đề nào trên chùm tia là quan trọng nữa. Tùy thuộc vào việc chúng ta rơi về phía nào, những vấn đề hoặc là sẽ được dễ dàng giải quyết hoặc là chúng ta chẳng còn vấn đề nào nữa vì người chết thì làm gì có vấn đề nào chứ.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Đó là lý do tại sao những người hiểu biết sâu sắc nhất về Siêu trí tuệ nhân tạo gọi nó là phát minh cuối cùng của chúng ta – thử thách cuối cùng mà chúng ta phải đối mặt.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Vậy thì chúng ta hãy cùng nói về nó.</span></p> 
<p style="text-align: justify;"><span style="color: #000000;"><strong>Nguồn:</strong></span></p> 
<p style="text-align: justify;"><span style="color: #000000;">Nếu bạn muốn đọc thêm về chủ đề này, bạn có thể tìm đọc thêm các bài báo dưới đây hoặc một trong những cuốn sách sau đây:</span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;"><strong>The most rigorous and thorough look at the dangers of AI:</strong></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;"><strong><br></strong>Nick Bostrom – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0199678111&amp;linkCode=as2&amp;tag=wabuwh00-20&amp;linkId=LBOTX2G2R72P5EUA" target="_blank">Superintelligence: Paths, Dangers, Strategies</a></strong></span></span></p> 
<p style="text-align: justify;"><span style="color: #000000;"><strong>The best overall overview of the whole topic and fun to read:</strong></span></p> 
<p style="text-align: justify;"><span style="color: #000000;"><strong><br></strong>James Barrat – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://www.amazon.com/gp/product/B00CQYAWRY/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=B00CQYAWRY&amp;linkCode=as2&amp;tag=wabuwh00-20&amp;linkId=3SF7IUFSRCKH7C4J" target="_blank">Our Final Invention</a></strong></span></span></p> 
<p style="text-align: justify;"><span style="color: #000000;"><strong>Controversial and a lot of fun. Packed with facts and charts and mind-blowing future projections:<br></strong>Ray Kurzweil – <a style="color: #000000;" href="https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0143037889&amp;linkCode=as2&amp;tag=wabuwh00-20&amp;linkId=54Q62R5PYJBEENTP" target="_blank"><span style="color: #0000ff;"><strong>The Singularity is Near</strong></span><br></a></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;"><strong>Articles and Papers:</strong></span></p> 
<p class="p1" style="text-align: justify;"><br><span style="color: #000000;">J. Nils Nilsson – <a style="color: #000000;" href="https://www.amazon.com/gp/product/0521122937/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0521122937&amp;linkCode=as2&amp;tag=wabuwh00-20&amp;linkId=QIJQME4U3J2KZRRY" target="_blank"><span style="color: #0000ff;"><strong>The Quest for Artificial Intelligence: A History of Ideas and Achievements</strong></span></a></span><span style="color: #000000;"><a style="color: #000000;" href="https://www.amazon.com/gp/product/0521122937/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0521122937&amp;linkCode=as2&amp;tag=wabuwh00-20&amp;linkId=QIJQME4U3J2KZRRY" target="_blank"><br></a>Steven Pinker – <a style="color: #000000;" href="https://www.amazon.com/gp/product/1491514965/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=1491514965&amp;linkCode=as2&amp;tag=wabuwh00-20&amp;linkId=NJ47RPDRBVZA6QPU" target="_blank"><span style="color: #0000ff;"><strong>How the Mind Works</strong></span></a></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Vernor Vinge – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html" target="_blank">The Coming Technological Singularity: How to Survive in the Post-Human Era</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Ernest Davis&nbsp;– <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www.cs.nyu.edu/faculty/davise/papers/Bostrom.pdf" target="_blank">Ethical Guidelines for A Superintelligence</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #0000ff;"><strong>Nick Bostrom – <a style="color: #0000ff;" href="http://www.nickbostrom.com/superintelligence.html" target="_blank">How Long Before Superintelligence?</a></strong></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Vincent C. Müller and Nick Bostrom – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www.nickbostrom.com/papers/survey.pdf" target="_blank">Future Progress in Artificial Intelligence: A Survey of Expert Opinion</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Moshe Y. Vardi – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://cacm.acm.org/magazines/2012/1/144824-artificial-intelligence-past-and-future/fulltext" target="_blank">Artificial Intelligence: Past and Future</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Russ Roberts, EconTalk – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www.econtalk.org/archives/2014/12/nick_bostrom_on.html" target="_blank">Bostrom Interview</a> and <a style="color: #0000ff;" href="http://www.econtalk.org/archives/2014/12/bostrom_follow-.html" target="_blank">Bostrom Follow-Up</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Stuart Armstrong and Kaj Sotala, MIRI – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://intelligence.org/files/PredictingAI.pdf" target="_blank">How We’re Predicting AI—or Failing To</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Susan Schneider – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://schneiderwebsite.com/Susan_Schneiders_Website/Research_files/12%20Schneider%20Newest-Alien%20Minds_1.pdf" target="_blank">Alien Minds</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Stuart Russell and Peter Norvig – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://amzn.to/1mHo9dG" target="_blank">Artificial Intelligence: A Modern Approach</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Theodore Modis – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www.growth-dynamics.com/articles/Kurzweil.htm" target="_blank">The Singularity Myth</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Gary Marcus – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www.newyorker.com/tech/elements/hyping-artificial-intelligence-yet-again" target="_blank">Hyping Artificial Intelligence, Yet Again</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Steven Pinker – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://users.manchester.edu/Facstaff/SSNaragon/Online/100-FYS-F15/Readings/Pinker,%20ConsciousComputers.pdf" target="_blank">Could a Computer Ever Be Conscious?</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Carl Shulman – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://intelligence.org/files/BasicAIDrives.pdf" target="_blank">Omohundro’s “Basic AI Drives” and Catastrophic Risks</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">World Economic Forum – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www3.weforum.org/docs/WEF_Global_Risks_2015_Report15.pdf" target="_blank">Global Risks 2015</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">John R. Searle – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www.nybooks.com/articles/archives/2014/oct/09/what-your-computer-cant-know/" target="_blank">What Your Computer Can’t Know</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Jaron Lanier – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://edge.org/conversation/one-half-a-manifesto" target="_blank">One Half a Manifesto</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Bill Joy – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://archive.wired.com/wired/archive/8.04/joy.html" target="_blank">Why the Future Doesn’t Need Us</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Kevin Kelly – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://kk.org/thetechnium/2008/09/thinkism/" target="_blank">Thinkism</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Paul Allen – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://www.technologyreview.com/view/425733/paul-allen-the-singularity-isnt-near/" target="_blank">The Singularity Isn’t Near</a> (and <a style="color: #0000ff;" href="https://www.technologyreview.com/view/425818/kurzweil-responds-dont-underestimate-the-singularity/" target="_blank">Kurzweil’s response</a>)</strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Stephen Hawking – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www.huffingtonpost.com/stephen-hawking/artificial-intelligence_b_5174265.html" target="_blank">Transcending Complacency on Superintelligent Machines</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Kurt Andersen – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www.vanityfair.com/culture/2014/11/artificial-intelligence-singularity-theory" target="_blank">Enthusiasts and Skeptics Debate Artificial Intelligence</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;"><a style="color: #000000;" href="http://longbets.org/1/" target="_blank">Terms of Ray Kurzweil and Mitch Kapor’s bet about the AI timeline</a></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Ben Goertzel – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://goertzel.org/TenYearsToTheSingularity.pdf" target="_blank">Ten Years To The Singularity If We Really Really Try</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Arthur C. Clarke – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="http://www.arthurcclarke.net/?scifi=3" target="_blank">Sir Arthur C. Clarke’s Predictions</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Hubert L. Dreyfus – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://www.amazon.com/gp/product/0262540673/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0262540673&amp;linkCode=as2&amp;tag=wabuwh00-20&amp;linkId=ZHBAVUQOM6SIGYHG" target="_blank">What Computers Still Can’t Do: A Critique of Artificial Reason</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Stuart Armstrong – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://www.amazon.com/gp/product/B00IB4N4KU/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=B00IB4N4KU&amp;linkCode=as2&amp;tag=wabuwh00-20&amp;linkId=FF3IC2DJNEHA5IAW" target="_blank">Smarter Than Us: The Rise of Machine Intelligence</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Ted Greenwald – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://www.wired.com/2012/06/mf_icons_diamandis/all/" target="_blank">X Prize Founder Peter Diamandis Has His Eyes on the Future</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #0000ff;"><strong>Kaj Sotala and Roman V. Yampolskiy – <a style="color: #0000ff;" href="https://intelligence.org/files/ResponsesAGIRisk.pdf%20" target="_blank">Responses to Catastrophic AGI Risk: A Survey</a></strong></span></p> 
<p class="p1" style="text-align: justify;"><span style="color: #000000;">Jeremy Howard TED Talk – <span style="color: #0000ff;"><strong><a style="color: #0000ff;" href="https://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn?language=en#t-5550" target="_blank">The wonderful and terrifying implications of computers that can learn</a></strong></span></span></p> 
<p class="p1" style="text-align: justify;">&nbsp;</p> 
<p class="p1" style="text-align: justify;">&nbsp;</p> 
<p>Đọc 6 phần ở đây:</p> 
<h4><span style="color: #0000ff;"><a style="color: #0000ff;" href="/cuoc-cach-mang-tri-tue-nhan-tao-con-duong-toi-sieu-tri-tue-phan-1">Cuộc cách mạng trí tuệ nhân tạo: Con đường tới Siêu trí tuệ (Phần 1)</a></span></h4> 
<h4><span style="color: #0000ff;"><a style="color: #0000ff;" href="/cuoc-cach-mang-tri-tue-nhan-tao-con-duong-toi-sieu-tri-tue-phan-2">Cuộc cách mạng trí tuệ nhân tạo: Con đường tới Siêu trí tuệ (Phần 2)</a></span></h4> 
<h4><span style="color: #0000ff;"><a style="color: #0000ff;" href="/cuoc-cach-mang-tri-tue-nhan-tao-nhan-loai-se-tro-nen-bat-tu-hay-diet-chung-phan-3">Cuộc cách mạng trí tuệ nhân tạo: Nhân loại sẽ trở nên bất tử hay diệt chủng? (Phần 3)</a></span></h4> 
<h4><span style="color: #0000ff;"><a style="color: #0000ff;" href="/cuoc-cach-mang-tri-tue-nhan-tao-nhan-loai-se-tro-nen-bat-tu-hay-diet-chung-phan-4">Cuộc cách mạng trí tuệ nhân tạo: Nhân loại sẽ trở nên bất tử hay diệt chủng? (Phần 4)</a></span></h4> 
<div class="composs-main-article-head"> 
 <h4><span style="color: #0000ff;"><a style="color: #0000ff;" href="/cuoc-cach-mang-tri-tue-nhan-tao-nhan-loai-se-tro-nen-bat-tu-hay-diet-chung-phan-5">Cuộc cách mạng trí tuệ nhân tạo: Nhân loại sẽ trở nên bất tử hay diệt chủng? (Phần 5)</a></span></h4> 
 <div class="composs-main-article-head"> 
  <h4><span style="color: #0000ff;"><a style="color: #0000ff;" href="/cuoc-cach-mang-tri-tue-nhan-tao-nhan-loai-se-tro-nen-bat-tu-hay-diet-chung-phan-6">Cuộc cách mạng trí tuệ nhân tạo: Nhân loại sẽ trở nên bất tử hay diệt chủng? (Phần 6)</a></span></h4> 
 </div> 
</div> 
<p><strong>Dịch</strong>:&nbsp;<a href="https://liemaximun1208.wordpress.com/2017/03/16/cuoc-cach-mang-tri-tue-nhan-tao-nhan-loai-se-tro-nen-bat-tu-hay-diet-chung/">https://liemaximun1208.wordpress.com/2017/03/16/cuoc-cach-mang-tri-tue-nhan-tao-nhan-loai-se-tro-nen-bat-tu-hay-diet-chung/</a></p> 
<p><strong>Nguồn</strong>:&nbsp;<a href="http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html">http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html</a></p>